---
title: "How to help LLMs understand your nbdev project"
date: 2024-01-20
author: "Radek Osmulski"
description: "A practical guide to making your nbdev library LLM-friendly using llms.txt, with automation techniques and implementation examples."
---

## Overview

Large Language Models can significantly enhance the developer experience, but they need concise, well-organized documentation to work effectively. This tutorial demonstrates how to add `llms.txt` to your nbdev project, creating a clear interface between your code and LLMs. You'll learn to generate `llms-ctx.txt` and `llms-ctx-full.txt` files and integrate them with your documentation.

While this guide focuses on nbdev, the underlying principles and tools are framework-agnostic and can help make any codebase more accessible to LLMs.

Let's explore how to implement this.

## The key ingredient: llms.txt

The foundation of LLM-friendly documentation is the `llms.txt` file. You can think of it as a GitHub README but designed specifically for LLMs. It starts with a high-level description followed by sections that include more and more lower level details.

Here is a minimal example:

```markdown
# FastHTML

> FastHTML is a python library which...

When writing FastHTML apps remember to:

- Thing to remember
```

### Tips on writing the description

Creating an effective description often begins with collecting existing content. Consider drawing from:
- Your project's README
- Blog posts and articles
- Social media discussions
- Technical documentation

The idea is to focus on clarity and accessibility. One effective approach is to frame your project as you would when introducing it to a new team member. This often yields the right balance of precision and comprehension.

Another technique that can speed you along the way is tapping an LLM for help. You can pass it content from multiple sources and ask it to help you synthetize it into a cohesive description. This can work particularly well because LLMs excel at summarizing and translating content from one format to another.

### Adding resource sections

The next step is to organize supplementary documentation into themed sections. Each section should contain links to resources that provide in-depth information about specific aspects of your project.

Markdown files are strongly recommended here as they offer a good balance of structure and readability. You could attempt linking to other formats, but your results may vary. For instance, HTML tends to be verbose, and formats like CSV rarely contain information that lends itself well to documenting functionality.

Here's an example of what this section can look like:

```markdown
## Docs

- [Surreal](https://host/README.md): Tiny jQuery alternative with Locality of Behavior
- [FastHTML quick start](https://host/quickstart.html.md): An overview of FastHTML features

## Examples

- [Todo app](https://host/adv_app.py)
```

### The Optional section

The `Optional` section provides a mechanism for managing context size. Resources listed in this section appear only in `llms-ctx-full.txt`, while being omitted from `llms-ctx.txt`. This allows for more flexibility and allows you to choose the right amount of context based on your use case and the capabilities of the LLM you plan to use.

```markdown
## Optional

- [Starlette docs](https://host/starlette-sml.md): A subset of the Starlette docs
```

Your `llms.txt` file is now complete. The next step is to automatically expand it by retrieving content from all the linked resources.

## Generating context files

The [llms-txt](https://llmstxt.org/intro.html) library automates the process of generating context files from your `llms.txt`. It can be used either through its CLI interface or as a Python module.

Using the CLI:

```bash
llms_txt2ctx llms.txt --save_nbdev_fname llms-ctx.txt
```

Or via Python:

```python
from llms_txt import *
samp = Path('llms-sample.txt').read_text()
parsed = parse_llms_file(samp)
```

Both approaches read your `llms.txt` file and retrieve the linked content.

Next, let's take a look at how we can use [pysymbol-llm](https://github.com/AnswerDotAI/pysymbol-llm) to add even more useful information in an automated way.

## Enhancing context with API reference

While LLMs generally understand high-level concepts, they often struggle with implementation details, especially when their training data is outdated. Providing a comprehensive list of your library's symbols - functions, classes, and their documentation - helps bridge this gap.

The [pysymbol-llm](https://github.com/AnswerDotAI/pysymbol-llm) library automates this process. It generates a complete API reference in Markdown format, extracting existing docstrings along the way.

For reference, here is the [apilist.txt](https://fastcore.fast.ai/apilist.txt) generated for [fastcore](https://github.com/fastai/fastcore). The tool scales well to larger libraries - for instance, generating the API reference for numpy requires just one command:

```bash
pysym2md numpy
```

To implement this in your project, generate an `apilist.txt`, serve it alongside your documentation, and reference it from your `llms.txt` file.

## Configuration

The final step is to configure your nbdev project to generate and serve these context files. This requires three changes:

1. Add your `llms.txt` file to the `nbs` directory of your project.

2. Add the required dependencies to `settings.ini`:
```
dev_requirements = pysymbol_llm llms-txt
```
3. Configure Quarto's build process in `nbs/_quarto.yml`:
```
project:
  type: website
  pre-render:
    - pysym2md --output_file apilist.txt nbdev
  post-render:
    - llms_txt2ctx llms.txt --optional true --save_nbdev_fname llms-ctx-full.txt
    - llms_txt2ctx llms.txt --save_nbdev_fname llms-ctx.txt
  resources:
    - "*.txt"
```

Remember to manually add a link to the generated `apilist.txt` in your `llms.txt` file. Once you commit these changes and rebuild your docs, your library will be ready for deeper, more accurate conversations with LLMs.

## Learning from examples

Working effectively with LLMs often involves studying successful implementations. The [fastcore](https://fastcore.fast.ai/llms.txt) project offers a good reference, and you can find additional examples in the [llmstxt.site](https://llmstxt.site/) and [llmstxt.cloud](https://directory.llmstxt.cloud/) directories.

To see all the necessary changes in one place, here's a [complete example](https://github.com/AnswerDotAI/nbdev/pull/1485/files) of adding `llms.txt` to an existing nbdev project.

Making your library LLM-friendly opens new possibilities for AI-assisted development and exploring topics in programming you might not have encountered before. We hope this guide helps you take advantage of these exciting new tools.